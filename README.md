
# **xml\_AI\_agent**

### **Context**

**Virome\@tlas** aims to build a global cloud-based platform for viral surveillance, integrating large-scale sequencing and geographic data to monitor virus diversity, hosts, and environments.
As part of this project, we focus on parsing massive SRA files to extract meaningful information.


### ‚ùó **Problem**

SRA (`.xml`) files often contain poorly annotated fields, especially the location. 


### üîé **Example data**

```xml
<Attribute attribute_name="Longitude">60 16' 10'' N</Attribute>
<Attribute attribute_name="Latitude">5 13' 20'' E</Attribute>
```

```xml
<Attribute attribute_name="lat_lon" harmonized_name="lat_lon" display_name="latitude and longitude">32.167 N 64.50 W</Attribute>
```

```xml
<Attribute attribute_name="geo_loc_name" harmonized_name="geo_loc_name" display_name="geographic location">USA: Oregon</Attribute>
```

```xml
<Attribute attribute_name="geo_loc_name" harmonized_name="geo_loc_name" display_name="geographic location">Kenya: Shimoni</Attribute>
<Attribute attribute_name="lat_lon" harmonized_name="lat_lon" display_name="latitude and longitude">NA</Attribute>
```

But usally the location is not provided of in the wrong field. 


### üí° **Idea**

Use a **Large Language Model (LLM)** to automatically extract this information from XML files.


## ü§ñ **Why use an LLM?**

A **Large Language Model (LLM)** is trained on vast amounts of text using self-supervised learning. It is designed for natural language processing tasks such as text generation, summarization, and classification.

‚úÖ **Advantages**

* Automates complex tasks (summarization, classification, content generation)
* Improves productivity and writing quality
* Enables interaction via natural language

‚ö† **Disadvantages**

* High environmental impact (hardware, training)
* Potential for biased outputs
* Data privacy and security risks (especially with private services like ChatGPT)
* Requires expertise and resources (for local deployment)


## Process (to be changed) 

**Using an LLM (via Ollama)**

Attempt to extract location data from `<BioSample>` using:

* **Transformers**
* **Python**
* **Hugging Face**


1Ô∏è‚É£ **Create a virtual environment**

```bash
python -m venv venv
source venv/bin/activate
```

2Ô∏è‚É£ **Install required packages**

```bash
pip install transformers accelerate huggingface_hub
```

3Ô∏è‚É£ **Authenticate Hugging Face CLI**

```bash
huggingface-cli login
```

(Enter your token to access gated models)

4Ô∏è‚É£ **Load the model**

* Use `transformers` to load the model directly from Hugging Face.

5Ô∏è‚É£ **Prepare XML data**

* Load and parse large XML files
* Split into smaller `<BioSample>` blocks for manageable prompts
* Feed the XML chunks to the model
* Collect and process the outputs (e.g., CSV)